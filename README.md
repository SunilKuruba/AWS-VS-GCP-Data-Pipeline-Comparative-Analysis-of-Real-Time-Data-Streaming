# Comparative Analysis of Realâ€‘Time Streaming Pipelines on AWS & GCP

## Introduction

Realâ€‘time decisionâ€‘making has become a competitive necessity as billions of IoT sensors, mobile apps, and online services emit continuous data.  Batchâ€‘oriented frameworks such as Hadoop can no longer keep pace with millisecondâ€‘level insight demands.  Managed cloud servicesâ€”e.g.\ Amazonâ€¯Kinesis, AWSâ€¯Lambda, GoogleÂ CloudÂ Pub/Subâ€‘backed Kafka, Dataflow, and Bigtableâ€”promise elastic scaling without operational overhead, but platform choice now hinges on more than raw throughput; cost efficiency and environmental sustainability are equally critical.

## Motivation

* **Performance PressureÂ âš¡**Â Fraud detection, predictive maintenance, and smartâ€‘city monitoring require subâ€‘second analytics.
* **Cost AwarenessÂ ðŸ’°**Â Cloud pricing can spike with alwaysâ€‘on streams; architects need clear \$â€‘perâ€‘event economics.
* **Sustainability GoalsÂ ðŸŒ±**Â Enterprises face ESG mandates and must quantify the carbon impact of their workloads.
* **Holistic ViewÂ ðŸ”**Â Most prior work benchmarks isolated components; real systems demand endâ€‘toâ€‘end evaluations.

## Features

* **Mirrorâ€‘image ETL stacks** built natively on each cloud (AWSÂ KinesisÂ â†’Â LambdaÂ â†’Â DynamoDB vs.\ GCPÂ KafkaÂ â†’Â BeamÂ â†’Â Bigtable).
* **Infrastructureâ€‘asâ€‘Code** with Terraform for oneâ€‘command deploy/destroy.
* **Parameterized load generator** to replay IoTâ€‘like telemetry at 1â€¯kâ€¯â€“â€¯50â€¯kÂ events/min.
* **Automated metric capture** for latency, throughput, cost, and kgâ€¯COâ‚‚â€‘eq.
* **Portable dashboards** (CloudWatch & CloudÂ Monitoring) for sideâ€‘byâ€‘side analytics.

## System Design

Both pipelines follow a threeâ€‘stage ETL pattern:

1. **IngestionÂ â†’** resilient message broker accepts raw JSON events.
2. **ProcessingÂ â†’** streaming engine enriches, aggregates, and validates data.
3. **StorageÂ â†’** lowâ€‘latency NoSQL store persists results for query/analytics.
   Identical sensor payloads and regional deployments (usâ€‘east) ensure fair comparison.

   ![AWS-GCP data-pipleine design](image.png)

### AWS Data Pipeline System Design

```
EC2 (Python publisher) â”€â–º Kinesis Data Streams â”€â–º AWS Lambda â”€â–º DynamoDB
```

* *EC2Â t3.medium* polls ThingSpeak or synthetic generator.
* *Kinesis* 1â€“N shards, 24â€¯h retention, atâ€‘leastâ€‘once delivery.
* *Lambda* PythonÂ 3.9, BatchSizeâ€¯=â€¯100, autoâ€‘scales to 1â€¯kÂ req/s.
* *DynamoDB* PAY\_PER\_REQUEST, singleâ€‘table with partition key `entry_id`.
  Endâ€‘toâ€‘end latency exported via custom CloudWatch metrics; cost & carbon via CUR and CustomerÂ CarbonÂ FootprintÂ Tool.

### GCP Data Pipeline System Design

```
ComputeÂ Engine VM â”€â–º ManagedÂ Kafka â”€â–º ApacheÂ BeamÂ (Dataflow) â”€â–º CloudÂ Bigtable
```

* *VMÂ e2â€‘standardâ€‘4* publishes adjustable event rate.
* *ManagedÂ Kafka* 3 brokers, 1+ partitions, exactlyâ€‘once semantics.
* *Beam on Dataflow* autoscaling (1â€“20 workers), slidingâ€‘window aggregations.
* *Bigtable* singleâ€‘cluster SSD, rowâ€‘key `deviceId#timestamp`.
  Metrics flow to CloudÂ Monitoring; Billing export + CarbonÂ Footprint API quantify \$ and COâ‚‚.

## Results (Key Takeaways)

* **Throughput:** AWS handled 15â€¯% higher bursty spikes; GCP sustained higher steadyâ€‘state (>â€¯50â€¯kâ€¯ev/min) without drops.
* **Latency:** Both <â€¯180â€¯ms at medium load; GCPâ€™s p95 latency 25â€“30â€¯% lower at peak due to Beam autoscaling.
* **Cost:** AWS cheapest for low/bursty traffic via Lambda billing; GCP \~18â€¯% cheaper at high throughput thanks to Dataflowâ€™s linear pricing.
* **Sustainability:** GCP offers granular carbon reporting and achieved higher carbonâ€‘free energy scores; AWS lacks serviceâ€‘level COâ‚‚ metrics.